# Apache Airflow for Boxing Data Pipeline

## ğŸ“ Directory Structure
```
airflow/
â”œâ”€â”€ docker/                 # Docker setup files
â”‚   â”œâ”€â”€ docker compose.yml  # Docker services configuration
â”‚   â”œâ”€â”€ .env                # Environment variables
â”‚   â””â”€â”€ setup.sh            # Initial setup script
â”œâ”€â”€ dags/                   # Your pipeline definitions
â”‚   â”œâ”€â”€ boxing_pipeline_docker.py
â”‚   â”œâ”€â”€ boxing_etl_dag.py
â”‚   â”œâ”€â”€ boxing_scraping_dag.py
â”‚   â””â”€â”€ test_dag.py
â”œâ”€â”€ logs/                   # Task execution logs
â”œâ”€â”€ plugins/                # Custom Airflow plugins
â””â”€â”€ README.md              # This file
```

## ğŸš€ Quick Start

### Prerequisites
1. Install Docker Desktop for Mac: https://www.docker.com/products/docker-desktop/
2. Make sure Docker is running (whale icon in menu bar)

### First Time Setup
```bash
# 1. Navigate to docker directory
cd /Users/devin/repos/projects/data-pipelines/airflow/docker

# 2. Run setup script
./setup.sh

# Wait for "Setup complete!"
```

### Start Airflow
```bash
# From the docker directory
cd /Users/devin/repos/projects/data-pipelines/airflow/docker

# Start all services
docker compose --env-file .env up -d

# Check status
docker compose ps
```

### Access Airflow UI
- URL: http://localhost:8080
- Username: `admin`
- Password: `admin123`

### Stop Airflow
```bash
# From the docker directory
cd /Users/devin/repos/projects/data-pipelines/airflow/docker

# Stop services
docker compose down
```

## ğŸ“Š Your DAGs (Pipelines)

| DAG Name | Description | Schedule |
|----------|-------------|----------|
| `boxing_pipeline_docker` | Main boxing data pipeline | Manual |
| `boxing_etl_dag` | ETL from data lake to staging | Daily |
| `boxing_scraping_dag` | Web scraping from BoxRec | Daily |
| `test_dag` | Simple test pipeline | Manual |

## ğŸ¯ How to Run a Pipeline

1. Open http://localhost:8080
2. Login with admin/admin123
3. Click on a DAG name
4. Click the Play button (â–¶ï¸)
5. Click "Trigger DAG"
6. Watch tasks turn green as they complete

## ğŸ”§ Common Commands

All commands should be run from the docker directory:
```bash
cd /Users/devin/repos/projects/data-pipelines/airflow/docker
```

### Check Status
```bash
docker compose ps
```

### View Logs
```bash
# All logs
docker compose logs -f

# Specific service
docker compose logs -f airflow-scheduler
```

### Restart Services
```bash
docker compose restart
```

### Run DAG via CLI
```bash
docker compose exec airflow-webserver airflow dags trigger boxing_pipeline_docker
```

### List DAGs
```bash
docker compose exec airflow-webserver airflow dags list
```

## â— Troubleshooting

### Docker Not Running
- Open Docker Desktop app
- Wait for whale icon in menu bar

### Can't Access UI
- Check Docker is running: `docker compose ps`
- Check logs: `docker compose logs airflow-webserver`
- Make sure port 8080 isn't in use

### DAGs Not Showing
- Check files are in `airflow/dags/`
- Wait 30 seconds for refresh
- Check for errors: `docker compose exec airflow-webserver airflow dags list-import-errors`

### Reset Everything
```bash
cd /Users/devin/repos/projects/data-pipelines/airflow/docker
docker compose down -v
./setup.sh
docker compose --env-file .env up -d
```

## ğŸ“ Configuration

### Environment Variables
Edit `airflow/docker/.env` to change:
- Admin username/password
- Python packages
- Database credentials

### Add New DAGs
1. Create Python file in `airflow/dags/`
2. Wait 30 seconds for auto-refresh
3. Check UI for new DAG

## ğŸ—ï¸ Architecture

- **PostgreSQL**: Metadata database
- **Webserver**: UI on port 8080
- **Scheduler**: Executes DAGs
- **LocalExecutor**: Runs tasks in parallel

## ğŸ“¦ What Was Cleaned Up

Removed standalone-only files:
- âŒ airflow.cfg (standalone config)
- âŒ airflow.db (SQLite database)
- âŒ start_airflow.sh (standalone script)
- âŒ init_airflow.sh (standalone init)
- âŒ generate_secrets.py (not needed)
- âŒ Old .env file

Everything is now Docker-based for consistency!